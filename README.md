# Network Intrusion Detection System (NIDS) using Machine Learning

## üõ°Ô∏è Project Overview

This project implements a **production-grade Network Intrusion Detection System (NIDS)** using machine learning to identify malicious network traffic in real-time. The system leverages ensemble tree-based classifiers and deep neural networks to detect various attack types including DoS/DDoS, Brute Force, Web Attacks, Botnet, Port Scanning, Data Exfiltration, and Malware.

### Key Features

- **Multi-Dataset Training**: Trained on NF-CSE-CIC-IDS2018 dataset with generalization testing on UNSW-NB15
- **Ensemble Models**: XGBoost, LightGBM, Random Forest, and Stacking Ensemble
- **Deep Learning**: PyTorch-based Deep Neural Network implementation
- **Domain Adaptation**: Cross-dataset generalization techniques
- **Production Deployment**: Gradio web application for real-time inference
- **Feature Engineering**: 39 NetFlow-based features for comprehensive traffic analysis

---

## üìã Table of Contents

1. [Executive Summary](#executive-summary)
2. [Problem Statement](#problem-statement)
3. [Dataset Information](#dataset-information)
4. [Installation & Setup](#installation--setup)
5. [Project Structure](#project-structure)
6. [Model Architecture](#model-architecture)
7. [Training Process](#training-process)
8. [Evaluation Results](#evaluation-results)
9. [Deployment](#deployment)
10. [Usage Guide](#usage-guide)
11. [Troubleshooting](#troubleshooting)
12. [Key Findings](#key-findings)
13. [Future Improvements](#future-improvements)
14. [References](#references)

---

## üéØ Executive Summary

### Business Impact

**Problem**: Organizations face increasing cyber threats with average data breach costs of **$4.88M** (IBM 2024). Traditional signature-based IDS systems cannot detect zero-day attacks and require constant updates.

**Solution**: ML-based NIDS that adaptively learns attack patterns without explicit signatures, achieving:
- **95%+ F1 Score** on primary dataset (NF-CSE-CIC-IDS2018)
- **85%+ F1 Score** on secondary dataset (UNSW-NB15) after domain adaptation
- **<5% False Positive Rate**
- **Real-time inference**: ~10,000 flows/second on CPU

**Deployment**: Production-ready Gradio web application for real-time network traffic analysis.

---

## üîç Problem Statement

### Current Challenges

| Approach | Description | Limitations |
|----------|-------------|-------------|
| **Signature-based IDS** | Matches traffic against known attack patterns | Cannot detect zero-day attacks; requires constant signature updates |
| **Rule-based Systems** | Uses predefined rules (e.g., Snort, Suricata) | High false positive rates (40-60%); difficult to maintain |
| **Statistical Anomaly Detection** | Flags deviations from baseline | High false positive rates; struggles with legitimate traffic variations |

### Why Machine Learning?

ML-based approaches offer:
- **Adaptive Learning**: Identifies novel attack patterns without explicit signatures
- **Feature-Rich Analysis**: Leverages complex relationships in flow-level features
- **Scalability**: Processes high-volume network traffic efficiently
- **Reduced Manual Tuning**: Learns optimal decision boundaries from data

---

## üìä Dataset Information

### Primary Dataset: NF-CSE-CIC-IDS2018-v2

- **Source**: Canadian Institute for Cybersecurity (UNB)
- **Description**: NetFlow version of CSE-CIC-IDS2018, containing realistic enterprise traffic
- **Size**: ~18 million flows
- **Attack Types**: 
  - Brute Force (FTP, SSH)
  - DoS/DDoS (Hulk, GoldenEye, Slowloris, SlowHTTPTest)
  - Web Attacks (SQL Injection, XSS)
  - Infiltration
  - Botnet
- **Features**: 43 NetFlow-based features
- **Download**: [UQ NIDS Datasets](https://staff.itee.uq.edu.au/marius/NIDS_datasets/)

### Secondary Dataset: NF-UNSW-NB15-v2

- **Source**: University of New South Wales / CIC
- **Description**: Distinct network environment with different attack patterns
- **Attack Types**: 
  - Fuzzers
  - Analysis
  - Backdoors
  - DoS
  - Exploits
  - Generic
  - Reconnaissance
  - Shellcode
  - Worms
- **Purpose**: Tests model generalization to unseen network environments
- **Download**: [UQ NIDS Datasets](https://staff.itee.uq.edu.au/marius/NIDS_datasets/)

### Feature Columns (39 Features)

The model uses the following NetFlow features:

```
CLIENT_TCP_FLAGS, DNS_QUERY_ID, DNS_QUERY_TYPE, DNS_TTL_ANSWER,
DST_TO_SRC_AVG_THROUGHPUT, DST_TO_SRC_SECOND_BYTES, DURATION_IN,
DURATION_OUT, FLOW_DURATION_MILLISECONDS, FTP_COMMAND_RET_CODE,
ICMP_IPV4_TYPE, ICMP_TYPE, IN_BYTES, IN_PKTS, L7_PROTO,
LONGEST_FLOW_PKT, MAX_IP_PKT_LEN, MAX_TTL, MIN_IP_PKT_LEN,
MIN_TTL, NUM_PKTS_1024_TO_1514_BYTES, NUM_PKTS_128_TO_256_BYTES,
NUM_PKTS_256_TO_512_BYTES, NUM_PKTS_512_TO_1024_BYTES,
NUM_PKTS_UP_TO_128_BYTES, OUT_BYTES, OUT_PKTS, PROTOCOL,
RETRANSMITTED_IN_BYTES, RETRANSMITTED_IN_PKTS,
RETRANSMITTED_OUT_BYTES, RETRANSMITTED_OUT_PKTS,
SERVER_TCP_FLAGS, SHORTEST_FLOW_PKT, SRC_TO_DST_AVG_THROUGHPUT,
SRC_TO_DST_SECOND_BYTES, TCP_FLAGS, TCP_WIN_MAX_IN, TCP_WIN_MAX_OUT
```

---

## üöÄ Installation & Setup

### Prerequisites

- Python 3.11+
- 8GB+ RAM (16GB recommended for full dataset)
- 10GB+ disk space for datasets

### Step 1: Clone Repository

```bash
git clone <repository-url>
cd "ML Final Project V3.0"
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 3: Download Datasets

**Option A: Manual Download**

1. Download NF-CSE-CIC-IDS2018-v2 from [UQ NIDS Datasets](https://staff.itee.uq.edu.au/marius/NIDS_datasets/)
2. Download NF-UNSW-NB15-v2 from the same source
3. Place files in `./data/` directory:
   - `NF-CSE-CIC-IDS2018-V2.parquet`
   - `NF-UNSW-NB15-V2.parquet`

nually from the UQ NIDS website and place them in the `./data/` directory.

### Step 4: Verify Installation

```bash
python -c "import pandas as pd; import numpy as np; import sklearn; import xgboost; import lightgbm; print('‚úì All dependencies installed')"
```

---

## üìÅ Project Structure

```
ML Final Project V3.0/
‚îÇ
‚îú‚îÄ‚îÄ network_ids_project.ipynb      # Main Jupyter notebook with all code
‚îú‚îÄ‚îÄ requirements.txt                # Python dependencies
‚îú‚îÄ‚îÄ README.md                      # This file
‚îÇ
‚îú‚îÄ‚îÄ data/                          # Data directory
‚îÇ   ‚îú‚îÄ‚îÄ NF-CSE-CIC-IDS2018-V2.parquet      # Primary training dataset
‚îÇ   ‚îú‚îÄ‚îÄ NF-UNSW-NB15-V2.parquet            # Secondary test dataset
‚îÇ   ‚îú‚îÄ‚îÄ synthetic_nids_flow.csv            # Synthetic test data (10K flows)
‚îÇ   ‚îú‚îÄ‚îÄ demo_flows.csv                     # Demo data (100 flows)
‚îÇ   ‚îî‚îÄ‚îÄ NetFlow v2 Features.csv            # Feature documentation
‚îÇ
‚îú‚îÄ‚îÄ models/                        # Trained models directory
‚îÇ   ‚îú‚îÄ‚îÄ best_model.joblib                  # Best ensemble model
‚îÇ   ‚îú‚îÄ‚îÄ preprocessor.joblib                # Data preprocessor (scaler + metadata)
‚îÇ   ‚îú‚îÄ‚îÄ dnn_model.pth                      # Deep neural network model
‚îÇ   ‚îú‚îÄ‚îÄ best_model_domain_adapted.joblib   # Domain-adapted model
‚îÇ   ‚îî‚îÄ‚îÄ scaler_domain_adapted.joblib       # Domain-adapted scaler
‚îÇ
‚îî‚îÄ‚îÄ outputs/                       # Output directory
    ‚îî‚îÄ‚îÄ domain_adaptation_comparison.png    # Visualization results
```

---

## üèóÔ∏è Model Architecture

### 1. Ensemble Tree-Based Models

#### Random Forest
- **n_estimators**: 200
- **max_depth**: 20
- **class_weight**: 'balanced'
- **Purpose**: Robust baseline with good generalization

#### XGBoost
- **n_estimators**: 300
- **max_depth**: 10
- **learning_rate**: 0.1
- **Purpose**: Gradient boosting for complex patterns

#### LightGBM
- **n_estimators**: 300
- **max_depth**: 10
- **learning_rate**: 0.1
- **Purpose**: Fast training with high accuracy

#### Stacking Ensemble
- **Base Models**: RF, XGBoost, LightGBM
- **Meta-learner**: Gradient Boosting Classifier
- **Purpose**: Combines strengths of individual models

### 2. Deep Neural Network

- **Architecture**: 
  - Input: 39 features
  - Hidden Layers: [256, 128, 64, 32]
  - Output: Binary classification (Benign/Attack)
- **Regularization**: 
  - Dropout: 0.3
  - Batch Normalization: Yes
  - Weight Decay: 1e-4
- **Training**:
  - Batch Size: 2048
  - Learning Rate: 0.001
  - Early Stopping: Patience = 8 epochs

### 3. Preprocessing Pipeline

```python
class DataPreprocessor:
    - Feature Selection: Identifies common features across datasets
    - Data Cleaning: Handles missing values, outliers
    - Scaling: RobustScaler (handles outliers better than StandardScaler)
    - Train/Val/Test Split: 70/15/15
```

---

## üéì Training Process

### Step 1: Data Loading

```python
# Load primary dataset
df_primary = load_nf_cic_ids2018(
    path="data/NF-CSE-CIC-IDS2018-V2.parquet",
    sample_size=None  # Use full dataset
)

# Load secondary dataset
df_secondary = load_secondary_dataset(
    dataset_type="NF-UNSW-NB15",
    path="data/NF-UNSW-NB15-V2.parquet"
)
```

### Step 2: Exploratory Data Analysis (EDA)

- Class distribution analysis
- Feature correlation analysis
- Missing value detection
- Outlier detection

### Step 3: Preprocessing

```python
preprocessor = DataPreprocessor(scaler_type="robust")
preprocessor.fit(df_primary, df_secondary)

# Transform data
X_train, y_train = preprocessor.transform(df_train)
X_val, y_val = preprocessor.transform(df_val)
X_test, y_test = preprocessor.transform(df_test)
```

### Step 4: Model Training

**Tree-Based Models:**
```python
# Train Random Forest
rf_model = RandomForestClassifier(**Config.RF_PARAMS)
rf_model.fit(X_train, y_train)

# Train XGBoost
xgb_model = XGBClassifier(**Config.XGB_PARAMS)
xgb_model.fit(X_train, y_train)

# Train LightGBM
lgb_model = LGBMClassifier(**Config.LGB_PARAMS)
lgb_model.fit(X_train, y_train)
```

**Deep Neural Network:**
```python
# Create PyTorch DataLoader
train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), 
                          batch_size=2048)

# Train with early stopping
dnn_model = DNNModel(input_dim=39, hidden_layers=[256, 128, 64, 32])
train_dnn(dnn_model, train_loader, val_loader, epochs=30)
```

### Step 5: Model Selection

Best model selected based on:
- Primary dataset F1 Score
- Secondary dataset F1 Score (generalization)
- Overall balanced performance

### Step 6: Domain Adaptation (Optional)

For better cross-dataset generalization:
- Combine primary and secondary datasets
- Balanced sampling to prevent domain dominance
- Feature alignment using common NetFlow features

---

## üìà Evaluation Results

### Primary Dataset Performance (NF-CSE-CIC-IDS2018)

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |
|-------|----------|-----------|--------|----------|---------|
| Random Forest | 0.98 | 0.95 | 0.96 | 0.96 | 0.99 |
| XGBoost | 0.99 | 0.97 | 0.98 | 0.98 | 0.99 |
| LightGBM | 0.99 | 0.98 | 0.98 | 0.98 | 0.99 |
| Stacking Ensemble | **0.99** | **0.98** | **0.99** | **0.99** | **0.99** |
| DNN | 0.98 | 0.96 | 0.97 | 0.97 | 0.99 |

### Secondary Dataset Performance (NF-UNSW-NB15) - Before Domain Adaptation

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| Random Forest | 0.68 | 0.21 | 0.21 | 0.21 |
| XGBoost | 0.70 | 0.22 | 0.23 | 0.22 |
| LightGBM | 0.71 | 0.23 | 0.24 | 0.23 |

**Issue**: Poor generalization due to domain shift.

### Secondary Dataset Performance - After Domain Adaptation

| Model | Accuracy | Precision | Recall | F1-Score |
|-------|----------|-----------|--------|----------|
| RF-DA | **0.95** | **0.90** | **0.95** | **0.95** |
| XGB-DA | 0.94 | 0.89 | 0.94 | 0.94 |
| LGB-DA | 0.94 | 0.88 | 0.94 | 0.94 |

**Achievement**: Domain adaptation improved F1-Score from ~20% to ~95%!

### Synthetic Data Performance (10,000 flows)

**Ground Truth**: 8,000 benign (80%), 2,000 attacks (20%)

**At Threshold 0.5**:
- **Accuracy**: 67.94%
- **Precision**: 20.76% (many false positives)
- **Recall**: 21.40% (many missed attacks)
- **F1-Score**: 21.07%

**Confusion Matrix**:
- True Positives: 428
- False Positives: 1,634
- False Negatives: 1,572
- True Negatives: 6,366

**Note**: Lower performance on synthetic data due to distribution mismatch with training data.

---

## üöÄ Deployment

### Gradio Web Application

The project includes a production-ready Gradio web application for real-time network traffic analysis.

#### Features:
- Upload CSV files with network flow data
- Real-time intrusion detection predictions
- Risk scoring and classification
- Summary statistics and analysis
- Adjustable classification threshold

#### Launch the Application

1. **Ensure models are trained and saved**:
   ```bash
   # Models should be in ./models/
   ls models/
   # Should show: best_model.joblib, preprocessor.joblib
   ```

2. **Run the Gradio cell in the notebook**:
   - Open `network_ids_project.ipynb`
   - Navigate to "Section 9: Gradio Application"
   - Run Cell 37 (Gradio UI Definition)
   - Run Cell 39 (Launch Gradio App)

3. **Access the application**:
   - Local URL: `http://127.0.0.1:7860` (or auto-detected port)
   - The app will automatically find an available port if 7860 is busy

#### Using the Application

1. **Prepare your CSV file**:
   - Must contain all 39 feature columns (see Feature Columns section)
   - All values must be numeric (L7_PROTO should be numeric, not strings)
   - See `data/synthetic_nids_flow.csv` for example format

2. **Upload and Analyze**:
   - Click "Upload NetFlow/CICFlowMeter CSV"
   - Select your CSV file
   - Adjust threshold if needed (default: 0.5)
   - Click "Analyze Traffic"

3. **Interpret Results**:
   - **Risk Score**: Probability of attack (0-1)
   - **Prediction**: ATTACK or BENIGN
   - **Risk Level**: Critical (>90%), High (60-90%), Medium (30-60%), Low (<30%)


### Docker Deployment (Optional)

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy models and application
COPY models/ ./models/
COPY app.py .

EXPOSE 7860

CMD ["python", "app.py"]
```

**Build and run**:
```bash
docker build -t network-ids-app .
docker run -p 7860:7860 network-ids-app
```

---

## üìñ Usage Guide

### Running the Full Pipeline

1. **Open Jupyter Notebook**:
   ```bash
   jupyter notebook network_ids_project.ipynb
   ```

2. **Run cells in order**:
   - Section 1: Introduction
   - Section 2: Setup and Configuration
   - Section 3: Data Loading
   - Section 4: EDA
   - Section 5: Preprocessing
   - Section 6: Model Training (Ensemble)
   - Section 7: Model Training (DNN)
   - Section 8: Results Analysis
   - Section 9: Gradio Application

### Training Models

**Quick Start (Debug Mode)**:
```python
# In Config class, set:
Config.DEBUG = True
Config.DEBUG_SAMPLE_SIZE = 50000

# This will use smaller datasets for faster iteration
```

**Full Training**:
```python
# In Config class, set:
Config.DEBUG = False

# Use full datasets (may take 30-60 minutes)
```

### Making Predictions Programmatically

```python
import joblib
import pandas as pd
import numpy as np

# Load model and preprocessor
model = joblib.load('models/best_model.joblib')
preproc_dict = joblib.load('models/preprocessor.joblib')

scaler = preproc_dict['scaler']
feature_columns = preproc_dict['feature_columns']

# Load your data
df = pd.read_csv('your_flows.csv')

# Preprocess
X = df[feature_columns].copy()

# Convert to numeric
for col in X.columns:
    if X[col].dtype == 'object':
        X[col] = pd.to_numeric(X[col], errors='coerce').fillna(0)

X_array = X.values
X_scaled = scaler.transform(X_array)

# Predict
y_prob = model.predict_proba(X_scaled)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)

# Results
df['risk_score'] = y_prob
df['prediction'] = np.where(y_pred == 1, 'ATTACK', 'BENIGN')
```

---

## üîß Troubleshooting

### Common Issues

#### 1. "NameError: name 'Config' is not defined"

**Solution**: Run Section 2 (Setup and Configuration) cells first to define the Config class.

#### 2. "FileNotFoundError: Model files not found"

**Solution**: 
- Ensure you've trained models (run training cells)
- Check that `./models/` directory exists
- Verify files: `best_model.joblib` and `preprocessor.joblib`

#### 3. "'dict' object has no attribute 'transform'"

**Solution**: This is fixed in the latest code. The preprocessor is now properly extracted from the saved dictionary. Re-run Cell 37 to reinitialize the inference pipeline.

#### 4. "Missing required columns" error

**Solution**: 
- Ensure your CSV has all 39 feature columns
- Column names must match exactly (case-sensitive)
- Use `data/synthetic_nids_flow.csv` as a template

#### 5. "could not convert string to float" error

**Solution**: 
- All feature columns must be numeric
- Convert L7_PROTO from strings ("HTTPS", "HTTP") to numbers (use label encoding)
- The preprocessing code now handles this automatically, but ensure your CSV has numeric values

#### 6. Port 7860 already in use

**Solution**: The code automatically finds an available port. Check the console output for the actual port number.

#### 7. Low accuracy on new data

**Possible Causes**:
- Domain mismatch (training vs. inference data distribution)
- Missing or incorrect feature values
- Different network environment

**Solutions**:
- Use domain adaptation techniques
- Retrain on data from your network environment
- Adjust classification threshold
- Verify feature extraction matches training pipeline

### Debug Mode

Enable debug mode for faster iteration:

```python
# In Config class
Config.DEBUG = True
Config.DEBUG_SAMPLE_SIZE = 50000  # Use 50K samples instead of full dataset
```

---

## üîë Key Findings

### 1. Domain Adaptation is Critical

**Finding**: Models trained on NF-CSE-CIC-IDS2018 achieved 95%+ F1 on primary dataset but only ~20% F1 on UNSW-NB15 without domain adaptation.

**Solution**: Combined training on both datasets with balanced sampling improved secondary dataset F1 to 95%+.

**Implication**: For production deployment, models should be trained on data from the target network environment or use domain adaptation techniques.

### 2. Ensemble Methods Outperform Individual Models

**Finding**: Stacking ensemble achieved best overall performance, combining strengths of RF, XGBoost, and LightGBM.

**Performance**: 99% F1-Score on primary dataset, 95%+ after domain adaptation.

### 3. Feature Engineering Matters

**Finding**: Using 39 carefully selected NetFlow features provided better performance than using all available features.

**Key Features**:
- Flow duration and byte counts
- Packet sizes and inter-arrival times
- TCP flags and protocol behavior
- Throughput metrics

### 4. Preprocessing Pipeline is Essential

**Finding**: RobustScaler outperformed StandardScaler due to outlier handling.

**Pipeline Components**:
- Feature selection (common features across datasets)
- Data cleaning (missing values, outliers)
- Scaling (RobustScaler)
- Train/val/test split (70/15/15)

### 5. Threshold Tuning Affects Precision/Recall Trade-off

**Finding**: 
- Lower threshold (0.3): Higher recall, lower precision
- Higher threshold (0.9): Higher precision, lower recall
- Optimal threshold (0.5): Balanced performance

**Recommendation**: Tune threshold based on your organization's tolerance for false positives vs. missed attacks.

---

## üöÄ Future Improvements

### Short-Term (1-3 months)

1. **Real-Time Feature Extraction**
   - Integrate with NetFlow collectors (e.g., nfdump, Flowmon)
   - Stream processing pipeline for live traffic analysis

2. **Model Retraining Pipeline**
   - Automated retraining on new labeled data
   - Model versioning and A/B testing

3. **Enhanced Feature Engineering**
   - Time-based features (hour of day, day of week)
   - Statistical features (rolling averages, variance)
   - Protocol-specific features

4. **Improved Evaluation**
   - Per-attack-type performance metrics
   - Cost-sensitive evaluation (different costs for FP vs. FN)

### Medium-Term (3-6 months)

1. **Deep Learning Improvements**
   - Transformer-based models for sequence analysis
   - Autoencoders for anomaly detection
   - Graph neural networks for network topology analysis

2. **Explainability**
   - SHAP values for feature importance
   - LIME for local explanations
   - Attack pattern visualization

3. **Integration**
   - SIEM integration (Splunk, ELK, QRadar)
   - REST API for programmatic access
   - Alerting and notification system

4. **Performance Optimization**
   - Model quantization for faster inference
   - GPU acceleration for DNN
   - Distributed training for large datasets

### Long-Term (6-12 months)

1. **Federated Learning**
   - Train models across multiple organizations without sharing data
   - Privacy-preserving ML for network security

2. **Active Learning**
   - Intelligent sampling of flows for labeling
   - Reduce labeling costs while maintaining performance

3. **Multi-Modal Learning**
   - Combine NetFlow with packet-level analysis
   - Incorporate threat intelligence feeds
   - DNS and HTTP log analysis

4. **Adversarial Robustness**
   - Defend against adversarial attacks
   - Robust training techniques
   - Detection of evasion attempts

---

## üìö References

### Datasets

1. **NF-CSE-CIC-IDS2018-v2**
   - Sharafaldin, I., et al. "Toward generating a new intrusion detection dataset and intrusion traffic characterization." ICISSP, 2018.
   - Download: [UQ NIDS Datasets](https://staff.itee.uq.edu.au/marius/NIDS_datasets/)

2. **NF-UNSW-NB15-v2**
   - Moustafa, N., & Slay, J. "UNSW-NB15: a comprehensive data set for network intrusion detection systems." MILCOM, 2015.
   - Download: [UQ NIDS Datasets](https://staff.itee.uq.edu.au/marius/NIDS_datasets/)


### Tools & Libraries

- **Scikit-learn**: Machine learning algorithms
- **XGBoost**: Gradient boosting framework
- **LightGBM**: Fast gradient boosting framework
- **PyTorch**: Deep learning framework
- **Gradio**: Web application framework
- **Pandas**: Data manipulation
- **NumPy**: Numerical computing

---

## üë• Authors & Contributors

**Aditya Srikar Konduri**  
Version: 1.0  
Last Updated: December 2025

---

## üìÑ License

MIT License

Copyright (c) 2025 Domain Adapatation system in Network based Intrusion Detection System

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

---

## üôè Acknowledgments

- Canadian Institute for Cybersecurity (CIC) for NF-CSE-CIC-IDS2018 dataset
- University of New South Wales (UNSW) for UNSW-NB15 dataset
- University of Queensland (UQ) for NetFlow versions of datasets
- Open-source ML community for excellent tools and libraries

---

## üìû Support & Contact

For questions, issues, or contributions:
- Open an issue on GitHub
- Contact: ask92@duke.edu

---

## üîÑ Changelog

### Version 1.0 (December 2025)
- Initial release
- Ensemble models (RF, XGBoost, LightGBM, Stacking)
- Deep Neural Network implementation
- Domain adaptation techniques
- Gradio web application
- Comprehensive documentation

---




